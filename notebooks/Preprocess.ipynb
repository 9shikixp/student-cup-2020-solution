{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/moriya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/moriya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "maketrans = str.maketrans\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed 固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 13\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df.jobflag.values - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキスト正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L26'''\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    \"\"\"Converts a text to a sequence of words (or tokens).\n",
    "    # Arguments\n",
    "        text: Input text (string).\n",
    "        filters: list (or concatenation) of characters to filter out, such as\n",
    "            punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n``,\n",
    "            includes basic punctuation, tabs, and newlines.\n",
    "        lower: boolean. Whether to convert the input to lowercase.\n",
    "        split: str. Separator for word splitting.\n",
    "    # Returns\n",
    "        A list of words (or tokens).\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    if sys.version_info < (3,):\n",
    "        if isinstance(text, unicode):\n",
    "            translate_map = dict((ord(c), unicode(split)) for c in filters)\n",
    "            text = text.translate(translate_map)\n",
    "        elif len(split) == 1:\n",
    "            translate_map = maketrans(filters, split * len(filters))\n",
    "            text = text.translate(translate_map)\n",
    "        else:\n",
    "            for c in filters:\n",
    "                text = text.replace(c, split)\n",
    "    else:\n",
    "        translate_dict = dict((c, split) for c in filters)\n",
    "        translate_map = maketrans(translate_dict)\n",
    "        text = text.translate(translate_map)\n",
    "\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python'''\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = list(map(text_to_word_sequence, train_df.description.values))\n",
    "test_sequence = list(map(text_to_word_sequence, test_df.description.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = frozenset(stopwords.words('english'))\n",
    "remove_sw = lambda x: [word for word in x if word not in stop_words]\n",
    "\n",
    "train_sequence = list(map(remove_sw, train_sequence))\n",
    "test_sequence = list(map(remove_sw, test_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = list(map(nltk.pos_tag, train_sequence))\n",
    "test_sequence = list(map(nltk.pos_tag, test_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatize = lambda x: [wnl.lemmatize(w, get_wordnet_pos(pos)) if get_wordnet_pos(pos) else w for w, pos in x]\n",
    "train_sequence = list(map(lemmatize, train_sequence))\n",
    "test_sequence = list(map(lemmatize, test_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter([word for seq in train_sequence for word in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = list(map(lambda x: [word for word in x if word_counter[word] >= 2], train_sequence))\n",
    "test_X = list(map(lambda x: [word for word in x if word_counter[word] >= 2], test_sequence))\n",
    "train_X = np.array(list(map(' '.join, train_X)))\n",
    "test_X = np.array(list(map(' '.join, test_X)))\n",
    "all_X = np.concatenate([train_X, test_X], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重複文の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2y = defaultdict(list)\n",
    "for x, y in zip(train_X, train_y):\n",
    "    x2y[x] += [y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undertake structure unstructured data {0, 1}\n",
      "use predictive model increase optimize customer experience revenue generation ad target business outcome {0, 1}\n",
      "collaboration work peer within team outside team help develop solution work leader customer location help advise best achieve devops goal {2, 3}\n",
      "effectively explain technical concept level organization include senior manager stakeholder {0, 1}\n",
      "guide mentor junior engineer serve team lead appropriate {1}\n",
      "select feature build optimize classifier use machine learning technique {0}\n",
      "document requirement use case design {2}\n",
      "perform test unit independent system performance {2}\n",
      "ass effectiveness accuracy new data source data gathering technique {0, 1}\n",
      "collaboration work peer within team outside team help develop solution work leader customer location help advise best achieve devops goal {2, 3}\n",
      "design develop code requirement specification {2}\n",
      "establish high level code quality write unit test participate code review reduce cyclomatic complexity remove code duplication debug software module {2}\n",
      "perform duty assign {2, 3}\n",
      "focus match business need solution utilize first hand knowledge customer requirement suggestion influence future direction evolution solution {2, 3}\n",
      "work collaboratively internal team member investigate fixing defect {2}\n",
      "focus match business need solution utilize first hand knowledge customer requirement suggestion influence future direction evolution solution {2, 3}\n",
      "analyze understand data source apis design develop method connect collect data different data source {1}\n",
      "strong analytical problem solve skill {1, 2}\n",
      "compare result various methodology recommend best technique stake holder {0, 1}\n",
      "assist cod execution engine {1}\n",
      "create algorithms extract information large multiparametric data set {0, 1}\n",
      "develop embed automated process predictive model validation deployment implementation {0, 1}\n",
      "use predictive model increase optimize customer experience revenue generation ad target business outcome {0, 1}\n",
      "analyze extract optimize relevant information large amount data help drive business decision product feature operational efficiency {1}\n",
      "analyze failure fix bug {2}\n",
      "contribute towards future roadmap {2}\n",
      "contribute functional technical design prototyping process design stage implementation include scenario design flow mapping {3}\n",
      "develops maintain code integrates software component fully functional software system {2}\n",
      "verify data quality ensure via data clean {1}\n",
      "compare result various methodology recommend best technique stake holder {0, 1}\n",
      "work cloud data architect define robust architecture cloud setup pipeline work flow {1}\n",
      "perform related task require {2, 3}\n",
      "ass effectiveness accuracy new data source data gathering technique {0, 1}\n",
      "design build deploy monitor support production deployment data science solution {1}\n",
      "design develop software system use scientific analysis mathematical model predict measure outcome consequence design {2}\n",
      "build software application across multiple platform response need stakeholder {2}\n",
      "work different business organization across company understand need help identify new opportunity {0}\n",
      "work project product manager achieve time cost delivery {2}\n",
      "participate project member agile scrum team include project estimation task creation drive complete task sprint timely manner {2}\n",
      "ability perform light travel requirement need meet business demand average 20 {3}\n",
      "develops maintain code integrates software component fully functional software system {2}\n",
      "ensures software standard meet {2}\n",
      "strong analytical problem solve skill {1, 2}\n",
      "constantly improve code quality test coverage {1}\n",
      "continuously learn grow new exist technical skill {2}\n",
      "ass effectiveness accuracy new data source data gathering technique {0, 1}\n",
      "perform related task require {2, 3}\n",
      "lead participate incident response engagement guide client forensic investigation contain security incident guide long term remediation recommendation {3}\n",
      "effectively explain technical concept level organization include senior manager stakeholder {0, 1}\n",
      "promote culture collaboration inclusiveness {2}\n",
      "develop embed automated process predictive model validation deployment implementation {0, 1}\n",
      "presentation team member management customer {2}\n",
      "become expert understands solar energy efficiency give ability show customer reduce energy cost additional cost {3}\n",
      "provide support production flight system {2}\n",
      "create detailed documentation digital prototype custom solution product assist customer partner implementation {2}\n",
      "create algorithms extract information large multiparametric data set {0, 1}\n",
      "actively participate agile scrum team size work effort decompose functionality iteratively deliver value {2}\n",
      "work closely data scientist ensure source data aggregate cleansed {1}\n",
      "strong team work require position demand work collaboration across several r team member {2}\n",
      "design develop service orient architecture solution construct managing service publish internal external consumer integrate complex database third party component {2}\n",
      "perform duty assign {2, 3}\n",
      "examine firewall web database log source identify evidence artifact malicious compromise activity {3}\n",
      "act independently pro actively seek uncover resolve issue {2}\n",
      "analysis perform position share within company support various initiative underway analytical need ultimately impact company's generation asset load resource customer view product service offer customer {0}\n",
      "maintain exist code make improvement increase maintainability performance scalability {1}\n",
      "undertake structure unstructured data {0, 1}\n"
     ]
    }
   ],
   "source": [
    "ctrain_X = []\n",
    "ctrain_y = []\n",
    "ids = []\n",
    "exist_sentence = {}\n",
    "\n",
    "for sentence_id, (x, y) in enumerate(zip(train_X, train_y)):\n",
    "    if len(set(x2y[x])) == 1 and exist_sentence.get(x) is None:\n",
    "        ids += [sentence_id]\n",
    "        ctrain_X += [x]\n",
    "        ctrain_y += [y]\n",
    "        exist_sentence[x] = y\n",
    "    else:\n",
    "        print(x, set(x2y[x]))\n",
    "\n",
    "ctrain_X = np.array(ctrain_X)\n",
    "ctrain_y = np.array(ctrain_y)\n",
    "ids = np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2931, 2865)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X), len(ctrain_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トピックモデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = CountVectorizer(stop_words='english')\n",
    "bow = bow_model.fit_transform(all_X)\n",
    "# 初回実行時のみ保存\n",
    "# joblib.dump(bow_model, '../models/bow_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = joblib.load('../models/bow_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 4674\n",
      "INFO:lda:vocab_size: 1882\n",
      "INFO:lda:n_words: 45848\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 2000\n",
      "INFO:lda:<0> log likelihood: -531515\n",
      "INFO:lda:<100> log likelihood: -323779\n",
      "INFO:lda:<200> log likelihood: -321363\n",
      "INFO:lda:<300> log likelihood: -320375\n",
      "INFO:lda:<400> log likelihood: -320513\n",
      "INFO:lda:<500> log likelihood: -319680\n",
      "INFO:lda:<600> log likelihood: -319636\n",
      "INFO:lda:<700> log likelihood: -319452\n",
      "INFO:lda:<800> log likelihood: -319734\n",
      "INFO:lda:<900> log likelihood: -318931\n",
      "INFO:lda:<1000> log likelihood: -319041\n",
      "INFO:lda:<1100> log likelihood: -319352\n",
      "INFO:lda:<1200> log likelihood: -318991\n",
      "INFO:lda:<1300> log likelihood: -318855\n",
      "INFO:lda:<1400> log likelihood: -319128\n",
      "INFO:lda:<1500> log likelihood: -319124\n",
      "INFO:lda:<1600> log likelihood: -318871\n",
      "INFO:lda:<1700> log likelihood: -318794\n",
      "INFO:lda:<1800> log likelihood: -318622\n",
      "INFO:lda:<1900> log likelihood: -318447\n",
      "INFO:lda:<1999> log likelihood: -318014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_N = 20 train time 18.24513292312622\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "n_iter = 2000\n",
    "start = time.time()\n",
    "lda_model = lda.lda.LDA(n_topics=n, n_iter=n_iter, random_state=0, refresh=100)\n",
    "lda_model.fit(bow)\n",
    "# 初回実行時のみ保存\n",
    "# joblib.dump(lda_model, '../models/lda_model_{}_{}iter.pkl'.format(n, n_iter))\n",
    "end = time.time()\n",
    "print(\"topic_N =\", str(n), \"train time\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = joblib.load('../models/lda_model_20_2000iter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = bow_model.transform(all_X)\n",
    "theta_docs_20 = lda_model.transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_theta = theta_docs_20[:len(train_X)]\n",
    "test_theta = theta_docs_20[len(train_X):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_topic = train_theta.argmax(axis=1)\n",
    "test_topic = test_theta.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['topic_id'] = train_topic\n",
    "test_df['topic_id'] = test_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_theta_df = pd.DataFrame(train_theta)\n",
    "train_theta_df.columns = [f'topic{i}' for i in range(train_theta.shape[1])]\n",
    "test_theta_df = pd.DataFrame(test_theta)\n",
    "test_theta_df.columns = [f'topic{i}' for i in range(test_theta.shape[1])]\n",
    "\n",
    "train_df = pd.concat([train_df, train_theta_df], axis=1)\n",
    "test_df = pd.concat([test_df, test_theta_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../processed_data/train_df_2865_topic_theta.csv', index=False)\n",
    "test_df.to_csv('../processed_data/test_df_topic_theta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validationの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df.jobflag.values - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2valn = {}\n",
    "for topic_id, count in ((573 * test_df.topic_id.value_counts(normalize=True) * 2 + 1) // 2).items():\n",
    "    topic2valn[topic_id] = int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train_df.id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = []\n",
    "y_val = []\n",
    "ids_val = []\n",
    "train_topic = train_df.topic_id.values\n",
    "for i in range(20):\n",
    "# for i in range(10):\n",
    "    sub_ids = train_ids[train_topic == i]\n",
    "    sub_train_y = train_y[train_topic == i]\n",
    "    valn = topic2valn[i]\n",
    "    shuffle_idx = np.random.permutation(len(sub_ids))\n",
    "    ids_train += list(sub_ids[shuffle_idx][valn:])\n",
    "    y_val += list(sub_train_y[shuffle_idx][:valn])\n",
    "    ids_val += list(sub_ids[shuffle_idx][:valn])\n",
    "\n",
    "ids_train = np.array(ids_train, dtype=int)\n",
    "y_val = np.array(y_val, dtype=int)\n",
    "ids_val = np.array(ids_val, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 73,  58,  63, 123], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = np.array([0.2314, 0.1833, 0.1982, 0.3872])\n",
    "val_n = Counter(y_val)[1] / test_rate[1]\n",
    "cat2valn = ((val_n * test_rate * 2 + 1) // 2).astype(np.int32)\n",
    "cat2valn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_val2 = []\n",
    "for i in range(4):\n",
    "    sub_ids = ids_val[y_val == i]\n",
    "    valn = cat2valn[i]\n",
    "    shuffle_idx = np.random.permutation(len(sub_ids))\n",
    "    ids_val2 += list(sub_ids[shuffle_idx][:valn])\n",
    "\n",
    "ids_val2 = np.array(ids_val2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train = train_df.id.apply(lambda x: x in ids_train)\n",
    "train_df[in_train].to_csv('../processed_data/train_df_2293_theta.csv', index=False)\n",
    "in_val = train_df.id.apply(lambda x: x in ids_val)\n",
    "train_df[in_val].to_csv('../processed_data/val_df_572_theta.csv', index=False)\n",
    "in_val2 = train_df.id.apply(lambda x: x in ids_val2)\n",
    "train_df[in_val2].to_csv('../processed_data/val2_df_309_theta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2293, 24), (572, 24), (309, 24))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[in_train].shape, train_df[in_val].shape, train_df[in_val2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_seed = [42, 346, 291, 241, 312, 150, 353, 310, 266, 188]\n",
    "for s in magic_seed:\n",
    "    train_df = pd.read_csv('../processed_data/train_df_2293_theta.csv')\n",
    "    mins = []\n",
    "    for k,v in Counter(train_df.jobflag).items():\n",
    "        mins.append(v)\n",
    "    mins = min(mins)\n",
    "    train_ids = []\n",
    "    for i in range(1,5):\n",
    "        train_ids += train_df[train_df[\"jobflag\"]==i].sample(mins,random_state=s).id.values.tolist()\n",
    "    train_df = train_df[train_df.id.apply(lambda x: x in set(train_ids))]\n",
    "    train_df.to_csv('../processed_data/train_df_1088_theta_seed_{}.csv'.format(s), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student-cup-2020",
   "language": "python",
   "name": "student-cup-2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
