{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "maketrans = str.maketrans\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import cuda\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed 固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 13\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../processed_data/val_df_572_theta.csv')\n",
    "val2_df = pd.read_csv('../processed_data/val2_df_309_theta.csv')\n",
    "test_df = pd.read_csv('../processed_data/test_df_topic_theta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tokens = [f'[{i}]' for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JobModel, self).__init__()\n",
    "        \n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            'roberta-base', output_hidden_states=True)    \n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            'roberta-base', config=config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 4, bias=False)\n",
    "        self.topic_classifier = nn.Linear(config.hidden_size, 20, bias=False)\n",
    "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        nn.init.normal_(self.topic_classifier.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task_id):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "        x = torch.stack([hs[-1][:, 0], hs[-2][:, 0], hs[-3][:, 0], hs[-4][:, 0]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        if task_id == 0:\n",
    "            ret = self.classifier(x)\n",
    "        elif task_id == 1:\n",
    "            ret = self.topic_classifier(x)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初回実行時のみ保存\n",
    "# トークンidの順番は，seed_everythingで固定できなかったので，実行する度に変動します．\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', additional_special_tokens=sorted(topic_tokens))\n",
    "# tokenizer.save_pretrained('../models/topic_tokenizer/')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../models/topic_tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トピックトークン付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = '[' + val_df.topic_id.map(str).values + '] </s> ' + val_df.description.values\n",
    "X_val2 = '[' + val2_df.topic_id.map(str).values + '] </s> ' + val2_df.description.values\n",
    "test_X = '[' + test_df.topic_id.map(str).values + '] </s> ' + test_df.description.values\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "X_val2 = np.array(X_val2)\n",
    "test_X = np.array(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_header_tr = ['id', 'description', 'jobflag', 'topic_id']\n",
    "drop_header_te = ['id', 'description', 'topic_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トピック確率取り出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test = test_df.drop(drop_header_te, axis=1).values\n",
    "theta_val2 = val2_df.drop(drop_header_tr, axis=1).values\n",
    "\n",
    "theta_test = np.array(theta_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = val_df.jobflag.values -1\n",
    "y_val2 = val2_df.jobflag.values -1\n",
    "y_val = np.array(y_val, dtype=int)\n",
    "y_val2 = np.array(y_val2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((572,), (572,)), ((309,), (309,)), ((1743,), (1743, 20)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_val.shape, y_val.shape), (X_val2.shape, y_val2.shape), (test_X.shape, theta_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s=42 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010633070953190327, kl_loss 0.012884687903410314, acc 0.3096774193548387, acc2 0.08251035361580121\n",
      "0.48426573426573427 0.43234763966628953\n",
      "0.5275080906148867 0.46132979134817675\n",
      "[0.3275961  0.12908778 0.19219736 0.35111876]\n",
      "Epoch 1 : ce_loss 0.007962502539157867, kl_loss 0.010752946328120924, acc 0.5689338235294118, acc2 0.22799744897959184\n",
      "0.5996503496503497 0.5574300743637313\n",
      "0.6019417475728155 0.5856095202445568\n",
      "[0.20711417 0.11474469 0.33849684 0.33964429]\n",
      "Epoch 2 : ce_loss 0.006678893230855465, kl_loss 0.009545911736960505, acc 0.6500920810313076, acc2 0.3311026131293818\n",
      "0.5699300699300699 0.5430696964261297\n",
      "0.6019417475728155 0.5797930562162068\n",
      "[0.25702811 0.14400459 0.25989673 0.33907057]\n",
      "s=346 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010740173980593681, kl_loss 0.012799751382271454, acc 0.30082796688132474, acc2 0.08415683774306662\n",
      "0.46153846153846156 0.4267738836136997\n",
      "0.4336569579288026 0.43307718602949624\n",
      "[0.1124498  0.37005164 0.42283419 0.09466437]\n",
      "Epoch 1 : ce_loss 0.007986112497746944, kl_loss 0.010684136301789673, acc 0.5666973321067157, acc2 0.2429072362129423\n",
      "0.5716783216783217 0.546836520155756\n",
      "0.598705501618123 0.5741389383247053\n",
      "[0.21170396 0.09179575 0.26965003 0.42685026]\n",
      "Epoch 2 : ce_loss 0.006548464298248291, kl_loss 0.009410485937988686, acc 0.6448942042318307, acc2 0.35001593879502707\n",
      "0.5576923076923077 0.5334927031764833\n",
      "0.5954692556634305 0.5736782239246585\n",
      "[0.25588067 0.14629948 0.25645439 0.34136546]\n",
      "s=291 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.011127486824989319, kl_loss 0.013129178977664996, acc 0.28518859245630174, acc2 0.07905642333439591\n",
      "0.5052447552447552 0.2374878025822024\n",
      "0.2524271844660194 0.17046854922941665\n",
      "[0.05220884 0.92197361 0.02581756]\n",
      "Epoch 1 : ce_loss 0.009265421889722347, kl_loss 0.01189629586783041, acc 0.47010119595216193, acc2 0.14631813834874083\n",
      "0.5944055944055944 0.5613361703454725\n",
      "0.6148867313915858 0.5980111981082855\n",
      "[0.21113024 0.12162937 0.31497418 0.35226621]\n",
      "Epoch 2 : ce_loss 0.007558654993772507, kl_loss 0.01066711241599212, acc 0.5983455882352942, acc2 0.234375\n",
      "0.5751748251748252 0.5543779103688219\n",
      "0.6051779935275081 0.5916568181571487\n",
      "[0.24039013 0.15433161 0.27596099 0.32931727]\n",
      "s=241 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010907082818448544, kl_loss 0.012994411794617928, acc 0.27205882352941174, acc2 0.07876275510204081\n",
      "0.46503496503496505 0.4393608681304859\n",
      "0.44983818770226536 0.4491953498753897\n",
      "[0.23121056 0.31726908 0.31210557 0.1394148 ]\n",
      "Epoch 1 : ce_loss 0.008489209227263927, kl_loss 0.010951753922469836, acc 0.5266544117647058, acc2 0.22417091836734693\n",
      "0.5437062937062938 0.5218596663948188\n",
      "0.5825242718446602 0.5670112329451458\n",
      "[0.19104991 0.23637407 0.26104418 0.31153184]\n",
      "Epoch 2 : ce_loss 0.006792575586587191, kl_loss 0.009529547704808083, acc 0.6415441176470589, acc2 0.31887755102040816\n",
      "0.5786713286713286 0.549978950454565\n",
      "0.6245954692556634 0.6051433647595128\n",
      "[0.20940906 0.14515204 0.27481354 0.37062536]\n",
      "s=312 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010822998359799385, kl_loss 0.01293401250447678, acc 0.30082796688132474, acc2 0.058335989799171185\n",
      "0.5052447552447552 0.4443478729992705\n",
      "0.5080906148867314 0.4791054496530005\n",
      "[0.08892714 0.27825588 0.37693632 0.25588067]\n",
      "Epoch 1 : ce_loss 0.008074929006397724, kl_loss 0.01085075496289536, acc 0.58195211786372, acc2 0.24410452517527087\n",
      "0.5594405594405595 0.5329201279098295\n",
      "0.5760517799352751 0.5550932318797013\n",
      "[0.22317843 0.17900172 0.25358577 0.34423408]\n",
      "Epoch 2 : ce_loss 0.006482685916125774, kl_loss 0.009408225615771574, acc 0.6761729530818767, acc2 0.3630857507172458\n",
      "0.5874125874125874 0.5530780291271112\n",
      "0.5889967637540453 0.5724011645156505\n",
      "[0.21858864 0.15949512 0.32243259 0.29948365]\n",
      "s=150 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.011030690744519234, kl_loss 0.012915895177116424, acc 0.26243093922651933, acc2 0.05640535372848948\n",
      "0.13286713286713286 0.10687792291565876\n",
      "0.24271844660194175 0.14532767621305714\n",
      "[5.73723465e-04 8.84107860e-01 1.15318417e-01]\n",
      "Epoch 1 : ce_loss 0.009426260367035866, kl_loss 0.01173897002973842, acc 0.44250229990800366, acc2 0.15588141536499842\n",
      "0.5664335664335665 0.518641656149898\n",
      "0.598705501618123 0.5561917228800101\n",
      "[0.28456684 0.07515777 0.26391279 0.37636259]\n",
      "Epoch 2 : ce_loss 0.00731408828869462, kl_loss 0.00998316761115279, acc 0.6117755289788408, acc2 0.3085750717245776\n",
      "0.5524475524475524 0.5307466626273967\n",
      "0.598705501618123 0.5827119034892129\n",
      "[0.22145726 0.19506598 0.26678141 0.31669535]\n",
      "s=353 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010996262542903423, kl_loss 0.01293168364579582, acc 0.24839006439742412, acc2 0.0653490596110934\n",
      "0.47202797202797203 0.16033254156769597\n",
      "0.20388349514563106 0.08467741935483872\n",
      "[1.]\n",
      "Epoch 1 : ce_loss 0.009509597904980183, kl_loss 0.011842718440752239, acc 0.45814167433302666, acc2 0.15301243226012112\n",
      "0.541958041958042 0.5155252203624328\n",
      "0.5728155339805825 0.5482641367629435\n",
      "[0.20826162 0.15031555 0.26965003 0.37177281]\n",
      "Epoch 2 : ce_loss 0.007357244845479727, kl_loss 0.010068574915158878, acc 0.6209751609935602, acc2 0.3066624163213261\n",
      "0.5821678321678322 0.5415243319054823\n",
      "0.5663430420711975 0.5508921450159117\n",
      "[0.19162364 0.1881813  0.35111876 0.26907631]\n",
      "s=310 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.011189102195203304, kl_loss 0.013072814860946368, acc 0.25666973321067155, acc2 0.05961109340133886\n",
      "0.47202797202797203 0.16033254156769597\n",
      "0.20388349514563106 0.08467741935483872\n",
      "[1.]\n",
      "Epoch 1 : ce_loss 0.009583262726664543, kl_loss 0.012008279014037296, acc 0.4204231830726771, acc2 0.15173732865795345\n",
      "0.5891608391608392 0.5429965299883319\n",
      "0.6019417475728155 0.5763529140258564\n",
      "[0.25645439 0.13310384 0.3373494  0.27309237]\n",
      "Epoch 2 : ce_loss 0.007432255893945694, kl_loss 0.010085625069129326, acc 0.6108555657773689, acc2 0.3012432260121135\n",
      "0.5874125874125874 0.547802689048555\n",
      "0.598705501618123 0.5720228928343172\n",
      "[0.24899598 0.12736661 0.28514056 0.33849684]\n",
      "s=266 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010631106793880463, kl_loss 0.01290512989524371, acc 0.30266789328426863, acc2 0.07969397513547975\n",
      "0.40734265734265734 0.40472287944485363\n",
      "0.5501618122977346 0.486673302366733\n",
      "[0.17670683 0.26620769 0.07917384 0.47791165]\n",
      "Epoch 1 : ce_loss 0.008216416463255882, kl_loss 0.01057318374484338, acc 0.5666973321067157, acc2 0.271597067261715\n",
      "0.5472027972027972 0.5211131015774915\n",
      "0.5889967637540453 0.5728167789812614\n",
      "[0.16695353 0.23981641 0.29317269 0.30005737]\n",
      "Epoch 2 : ce_loss 0.006695195566862822, kl_loss 0.00924481248520159, acc 0.6629834254143646, acc2 0.3604206500956023\n",
      "0.5716783216783217 0.5427442059667563\n",
      "0.6051779935275081 0.5865349914061885\n",
      "[0.26620769 0.17555938 0.26735513 0.2908778 ]\n",
      "s=188 --------------------------------------------------\n",
      "Epoch 0 : ce_loss 0.010899698361754417, kl_loss 0.012968369585638531, acc 0.25390984360625574, acc2 0.06184252470513229\n",
      "0.10839160839160839 0.05613072891634091\n",
      "0.19093851132686085 0.08354531750518183\n",
      "[0.99139415 0.00229489 0.00631096]\n",
      "Epoch 1 : ce_loss 0.009111986495554447, kl_loss 0.011585666801328215, acc 0.44106813996316757, acc2 0.1864244741873805\n",
      "0.493006993006993 0.46757727676160077\n",
      "0.5210355987055016 0.4994130813424863\n",
      "[0.14687321 0.26333907 0.25932301 0.33046472]\n",
      "Epoch 2 : ce_loss 0.007059299387037754, kl_loss 0.009927293814183308, acc 0.6338546458141674, acc2 0.3092126235256615\n",
      "0.5804195804195804 0.5409559706619351\n",
      "0.5792880258899676 0.5601948351218922\n",
      "[0.26735513 0.09982788 0.30407344 0.32874355]\n"
     ]
    }
   ],
   "source": [
    "# 訓練データの削り方によって，得られるモデルからの予測カテゴリ割合が大きく変わるので，\n",
    "# RoBERTa シングルモデルの学習時に，暫定スコアが最も高い提出に予測カテゴリ割合が近くなるようなseedの選択\n",
    "magic_seed = [42, 346, 291, 241, 312, 150, 353, 310, 266, 188]\n",
    "for s in magic_seed:\n",
    "    train_df = pd.read_csv('../processed_data/train_df_1088_theta_seed_{}.csv'.format(s))\n",
    "    X_train = '[' + train_df.topic_id.map(str).values + '] </s> ' + train_df.description.values\n",
    "    theta_train = train_df.drop(drop_header_tr, axis=1).values\n",
    "    y_train = train_df.jobflag.values -1\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train, dtype=int)\n",
    "\n",
    "    all_X = np.concatenate([X_train, X_val2, test_X], axis=0)\n",
    "    all_theta = np.concatenate([theta_train, theta_val2, theta_test], axis=0)\n",
    "    all_ids = np.concatenate([np.arange(len(X_train)), np.arange(len(all_X))], axis=0).astype(np.int32)\n",
    "    task_ids = np.concatenate([np.zeros(len(X_train)), np.ones(len(all_X))], axis=0).astype(np.int32)\n",
    "    model = JobModel()\n",
    "    model.roberta.resize_token_embeddings(len(tokenizer))\n",
    "    device_id = 0\n",
    "    model.to(device_id)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "    batchsize = 64\n",
    "    num_iter = math.floor(len(all_ids) / batchsize)\n",
    "    num_epoch = 3\n",
    "\n",
    "    num_warmup_steps = 0\n",
    "    num_train_steps = num_iter * num_epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
    "    print(f'{s=}', '-'*50)\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        shuffled = np.random.permutation(len(all_ids))\n",
    "\n",
    "        sum_ce_loss = 0.0\n",
    "        sum_kl_loss = 0.0\n",
    "        n=0\n",
    "        cat_n = 0\n",
    "        topic_n = 0\n",
    "        sum_cat_correct = 0\n",
    "        sum_topic_correct = 0\n",
    "        val_correct = 0\n",
    "        batchsize = 64\n",
    "        for i in range(0, len(all_ids)-batchsize, batchsize):\n",
    "            ids = shuffled[i:i+batchsize]\n",
    "            xid, tid = all_ids[ids], task_ids[ids]\n",
    "            optimizer.zero_grad()\n",
    "            for task in range(2):\n",
    "                if task == 0:\n",
    "                    text_batch = list(X_train[xid[tid==task]])\n",
    "                    encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "                    input_ids = encoding['input_ids'].to(device_id)\n",
    "                    attention_mask = encoding['attention_mask'].to(device_id)\n",
    "                    labels = torch.tensor(y_train[xid[tid==task]]).to(device_id)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, task_id=task)\n",
    "\n",
    "                    ce_loss = 0.5 * F.cross_entropy(outputs, labels)\n",
    "                    true_y = labels.cpu().detach().numpy()\n",
    "                    pred_y = outputs.cpu().detach().numpy().argmax(axis=1)\n",
    "                    sum_cat_correct += np.sum(true_y == pred_y)\n",
    "                    cat_n += len(true_y)\n",
    "\n",
    "                elif task == 1:\n",
    "                    text_batch = list(all_X[xid[tid==task]])\n",
    "                    encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "                    input_ids = encoding['input_ids'].to(device_id)\n",
    "                    attention_mask = encoding['attention_mask'].to(device_id)\n",
    "                    labels = torch.tensor(all_theta[xid[tid==task]]).to(device_id)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, task_id=task)\n",
    "\n",
    "\n",
    "                    kl_loss = 0.5 * (- torch.sum(labels*F.log_softmax(outputs, dim=1), dim=1) + torch.sum(labels*torch.log(labels), dim=1)).mean()\n",
    "                    true_y = labels.cpu().detach().numpy().argmax(axis=1)\n",
    "                    pred_y = outputs.cpu().detach().numpy().argmax(axis=1)\n",
    "                    sum_topic_correct += np.sum(true_y == pred_y)\n",
    "                    topic_n += len(true_y)\n",
    "\n",
    "            loss = ce_loss + kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            sum_ce_loss += ce_loss.data\n",
    "            sum_kl_loss += kl_loss.data\n",
    "\n",
    "            n += len(ids)\n",
    "\n",
    "        accuracy_cat = sum_cat_correct / cat_n\n",
    "        accuracy_topic = sum_topic_correct / topic_n\n",
    "        print(\"Epoch {} : ce_loss {}, kl_loss {}, acc {}, acc2 {}\".format(epoch, sum_ce_loss / n, sum_kl_loss / n, accuracy_cat, accuracy_topic))\n",
    "        \n",
    "        # validation\n",
    "        val_pred = []\n",
    "        model.eval()\n",
    "        for i in range(0, len(X_val), batchsize):\n",
    "            text_batch = list(X_val[i:i+batchsize])\n",
    "            encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device_id)\n",
    "            attention_mask = encoding['attention_mask'].to(device_id)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, task_id=0)\n",
    "            pred_y = outputs.cpu().detach().numpy().argmax(axis=1)\n",
    "\n",
    "            val_pred += list(pred_y)\n",
    "            val_correct += np.sum(y_val[i:i+batchsize] == pred_y)\n",
    "        val_pred = np.array(val_pred)\n",
    "        print(val_correct/len(X_val), f1_score(y_val, val_pred, average='macro'))\n",
    "\n",
    "        val_correct = 0\n",
    "        val_pred = []\n",
    "        for i in range(0, len(X_val2), batchsize):\n",
    "            text_batch = list(X_val2[i:i+batchsize])\n",
    "            encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device_id)\n",
    "            attention_mask = encoding['attention_mask'].to(device_id)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, task_id=0)\n",
    "            pred_y = outputs.cpu().detach().numpy().argmax(axis=1)\n",
    "\n",
    "            val_pred += list(pred_y)\n",
    "            val_correct += np.sum(y_val2[i:i+batchsize] == pred_y)\n",
    "        val_pred = np.array(val_pred)\n",
    "        print(val_correct/len(X_val2), f1_score(y_val2, val_pred, average='macro'))\n",
    "        \n",
    "        # test に対する予測カテゴリの割合確認\n",
    "        model.eval()\n",
    "        batchsize = 32\n",
    "        test_y = []\n",
    "        for i in range(0, len(test_X), batchsize):\n",
    "            text_batch = list(test_X[i:i+batchsize])\n",
    "            encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            input_ids = encoding['input_ids'].to(device_id)\n",
    "            attention_mask = encoding['attention_mask'].to(device_id)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, task_id=0)\n",
    "            pred_y = outputs.cpu().detach().numpy().argmax(axis=1)\n",
    "            test_y += list(pred_y)\n",
    "\n",
    "        test_y = np.array(test_y)\n",
    "        test_y += 1\n",
    "        print(np.array(list(Counter(sorted(test_y)).values())) / len(test_y))\n",
    "    torch.save(model.state_dict(), '../models/roberta-10ens/roberta_mtdnn_ce5kl5_seed{}.model'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student-cup-2020",
   "language": "python",
   "name": "student-cup-2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
